对比学习（Contrastive Learning）是一种**自监督学习**方法，旨在通过让模型学习区分相似（正样本）和不相似（负样本）的数据来提取有意义的特征表示。其核心思想是：**在特征空间中拉近相似样本的距离，推远不相似样本的距离**。

---

### **核心概念**
1. **正样本（Positive Pair）**  
   - 通过数据增强（如裁剪、旋转、加噪等）从同一原始样本生成的不同视图，或语义相似的样本（如同一类别的图像）。
   - 例如：同一张图片的两次随机裁剪被视为正样本对。

2. **负样本（Negative Pair）**  
   - 不同原始样本或其增强版本（如不同类别的图像）。
   - 例如：来自不同图片的裁剪区域被视为负样本对。

3. **对比损失函数**  
   常用的损失函数（如InfoNCE）会最大化正样本对的相似度，最小化负样本对的相似度：  
   \[
   \mathcal{L} = -\log \frac{\exp(sim(z_i, z_j)/\tau)}{\sum_{k\neq i} \exp(sim(z_i, z_k)/\tau)}
   \]
   - \(z_i, z_j\) 是正样本对的特征向量。
   - \(sim(\cdot)\) 是相似度度量（如余弦相似度）。
   - \(\tau\) 是温度超参数，控制分布的尖锐程度。

---

### **关键步骤**
1. **数据增强**  
   生成同一输入的不同视图（如图像的颜色扭曲、旋转、遮挡等），确保正样本对具有语义一致性。

2. **编码器（Encoder）**  
   用神经网络（如ResNet、ViT）将输入映射到低维特征空间。

3. **投影头（Projection Head）**  
   一个小型网络（如MLP）将编码后的特征进一步映射到对比学习空间（通常后续任务中会丢弃此部分）。

4. **对比优化**  
   通过损失函数调整特征空间，使正样本对特征靠近，负样本对特征远离。

---

### **典型方法**
- **SimCLR**（Simple Contrastive Learning）  
  使用大批量样本中的负样本对，结合数据增强和投影头。
- **MoCo**（Momentum Contrast）  
  引入动态字典和动量编码器，缓解对大batch的依赖。
- **CLIP**（Contrastive Language-Image Pretraining）  
  跨模态对比学习，对齐图像和文本特征。

---

### **优势**
- **无需人工标注**：利用数据本身生成监督信号。
- **通用特征表示**：学到的特征可迁移到下游任务（如分类、检测）。
- **对数据增强鲁棒**：增强的多样性提升模型泛化能力。

---

### **应用场景**
- 计算机视觉（图像分类、目标检测）。
- 自然语言处理（句子嵌入）。
- 跨模态学习（图文匹配）。

对比学习通过“对比”相似与不相似样本，使模型捕捉数据中的本质特征，成为自监督学习的重要范式。

[paper](https://www.mdpi.com/2227-7080/9/1/2)